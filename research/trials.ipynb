{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x:\\\\DL\\\\Projects\\\\MLOPs'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"x:\\DL\\Projects\\MLOPs\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data: Path\n",
    "    unzip_dir: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    current_dset: Path\n",
    "    root_dir: Path\n",
    "    status_file_dir: Path\n",
    "    req_files: list\n",
    "    \n",
    "@dataclass(frozen=True)\n",
    "class TrainLogConfig:\n",
    "    model: str\n",
    "    save_path: Path\n",
    "    mlflow_uri: str\n",
    "    experiment_name: str\n",
    "    model_name: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Params:\n",
    "    optimizer: str\n",
    "    lr0: float\n",
    "    save_period: int\n",
    "    batch: int\n",
    "    epochs: int\n",
    "    resume: bool\n",
    "    seed: int\n",
    "    imgsz: int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.MLOPs.constants import *\n",
    "from scripts.MLOPs.utils.common import read_yaml, create_directories\n",
    "#from scripts.MLOPs.entity.config_entity import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_dataingestion_config(self)-> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        create_directories([config.root_dir])\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            source_URL= config.source_URL,\n",
    "            local_data= config.local_data,\n",
    "            unzip_dir= config.unzip_dir\n",
    "            )\n",
    "        return data_ingestion_config\n",
    "    \n",
    "    def get_datavalidation_config(self)->DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "        create_directories([config.data_val_dir])\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            current_dset= config.current_dset,\n",
    "            root_dir=config.data_val_dir,\n",
    "            status_file_dir= config.data_val_status,\n",
    "            req_files= config.data_val_req\n",
    "            )\n",
    "        return data_validation_config\n",
    "    \n",
    "    def get_train_log_config(self)-> TrainLogConfig:\n",
    "        config = self.config.train_log_config\n",
    "        trainlogconfig = TrainLogConfig(\n",
    "            model= config.model,\n",
    "            save_path= config.save_path,\n",
    "            mlflow_uri= config.mlflow_uri,\n",
    "            experiment_name= config.experiment_name,\n",
    "            model_name= config.model_name\n",
    "        )\n",
    "        return trainlogconfig\n",
    "    \n",
    "    def get_params(self)-> Params:\n",
    "        param = self.config.param\n",
    "        params = Params(\n",
    "            optimizer = param.optimizer,\n",
    "            lr0 = param.lr0,\n",
    "            save_period = param.save_period,\n",
    "            batch = param.batch,\n",
    "            epochs = param.epochs,\n",
    "            resume = param.resume,\n",
    "            seed = param.seed,\n",
    "            imgsz = param.imgsz\n",
    "        )\n",
    "        return params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from ultralytics import YOLO\n",
    "from scripts.MLOPs.utils.common import get_highest_train_folder\n",
    "\n",
    "\n",
    "class YoloWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        self.results = None\n",
    "        self.results_df = None\n",
    "        self.data = None\n",
    "\n",
    "    def load_context(self, context: object):\n",
    "        \"\"\"Load Yolo model from context path\n",
    "\n",
    "        Args:\n",
    "            context (object): An MLFlow object that is used to define the path to the model.\n",
    "        \"\"\"\n",
    "        runs_f = get_highest_train_folder(\"runs/detect\")\n",
    "        #logging.info(f\"artifacts[path]: runs/detect/{runs_f}/weights/best.pt\")\n",
    "        self.model = YOLO(f\"runs/detect/{runs_f}/weights/best.pt\")\n",
    "\n",
    "    def reformat_data(self):\n",
    "        \"\"\"Reformat given dictionary object that was coerced into numpy arrays into str,int,flow\"\"\"\n",
    "        # For each key-value pair, convert value to string.\n",
    "        for key, value in self.data.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                self.data[key] = \",\".join(map(str, [value]))\n",
    "\n",
    "        # For each key-value pair, convert value to appropriate type.\n",
    "        for key, value in self.data.items():\n",
    "            if value.isnumeric():  # Check if the value is an integer\n",
    "                self.data[key] = int(value)\n",
    "            elif value.replace(\".\", \"\", 1).isdigit():  # Check if the value is a float\n",
    "                self.data[key] = float(value)\n",
    "            elif value.lower() in [\"true\", \"false\"]:  # Check if the value is a boolean\n",
    "                self.data[key] = value.lower() == \"true\"\n",
    "\n",
    "    def yolo_results_to_df(self):\n",
    "        \"\"\"Create Yolo results as a df\"\"\"\n",
    "        # Retrieve bounding boxes\n",
    "        boxes = self.results[0].boxes\n",
    "        # Map class to string names\n",
    "        names = []\n",
    "        for object_class in boxes.cls.numpy():\n",
    "            names.append(self.model.names[object_class])\n",
    "        # Create return df\n",
    "        self.results_df = pd.DataFrame(\n",
    "            np.c_[boxes.xyxy.numpy(), boxes.conf, boxes.cls.numpy(), np.array(names)],\n",
    "            columns=[\"X1\", \"Y1\", \"X2\", \"Y2\", \"conf\", \"cls\", \"names\"],\n",
    "        )\n",
    "\n",
    "    def predict(self, context: object, data: dict):\n",
    "        \"\"\"Wrapper function around Yolo's predict function. Results are returned as a pandas dataframe.\n",
    "\n",
    "        Args:\n",
    "            context (object): An MLFlow object that is used to define the path to the model.\n",
    "            data (dict): dictionary with source for inference and override parameters for the model.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        logging.info(f\"Data input: f{self.data}\")\n",
    "        # Reformat data\n",
    "        self.reformat_data()\n",
    "        logging.info(f\"Data after reformat: f{self.data}\")\n",
    "\n",
    "        # Pass inputs to predict\n",
    "        self.results = self.model.predict(**self.data)\n",
    "        # Transform results to pandas df\n",
    "        self.yolo_results_to_df()\n",
    "\n",
    "        return self.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Registers model in MLFlow\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import logging\n",
    "import yaml\n",
    "import cloudpickle\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "# from scripts.mlflow_utils import model_wrapper\n",
    "# from scripts.mlflow_utils.model_wrapper import YoloWrapper\n",
    "\n",
    "\n",
    "def get_experiment_id(name: str):\n",
    "    \"\"\"Retrieve experiment if registered name, else create experiment.\n",
    "\n",
    "    Args:\n",
    "        name (str): Mlflow experiment name\n",
    "\n",
    "    Returns:\n",
    "        str: Mlfow experiment id\n",
    "    \"\"\"\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "        exp_id = mlflow.create_experiment(name)\n",
    "        return exp_id\n",
    "    return exp.experiment_id\n",
    "\n",
    "\n",
    "def read_lines(path: str):\n",
    "    \"\"\"Given a path to a file, this function reads file, seperates the lines and returns a list of those separated lines.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to file\n",
    "\n",
    "    Returns:\n",
    "        list: List made of of file lines\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "\n",
    "def log_metrics(save_dir: str, log_results: bool = True):\n",
    "    \"\"\"Log metrics to Mlflow from the Yolo model outputs.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): Path to Yolo save directory, i.e - runs/train\n",
    "        log_results (bool): If True, the results are logged to MLflow server\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    try:\n",
    "        with open(save_dir / \"results.csv\", \"r\") as csv_file:\n",
    "            metrics_reader = csv.DictReader(csv_file)\n",
    "            metrics_list = []\n",
    "            for metrics in metrics_reader:\n",
    "                # Create an empty dictionary to store the updated key-value pairs for this row\n",
    "                updated_metrics = {}\n",
    "                # Iterate through the key-value pairs in this row's dictionary\n",
    "                for key, value in metrics.items():\n",
    "                    # Remove whitespace from the key\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    # Remove extra strings in keys\n",
    "                    patterns = [\"(B)\", \"metrics/\"]\n",
    "                    for pattern in patterns:\n",
    "                        key = key.replace(pattern, \"\")\n",
    "                    # Add the updated key-value pair to the updated row dictionary\n",
    "                    try:\n",
    "                        # Add the updated key-value pair to the updated row dictionary\n",
    "                        updated_metrics[key] = float(value)\n",
    "                    except ValueError:\n",
    "                        logging.error(f\"ValueError: Could not convert {value} to float.\")\n",
    "                    metrics_list.append(updated_metrics)\n",
    "                    if log_results:\n",
    "                        mlflow.log_metrics(updated_metrics)\n",
    "        return metrics_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FileNotFoundError: Could not find {save_dir / 'results.csv'}.\")\n",
    "    except IOError:\n",
    "        print(f\"IOError: Could not read {save_dir / 'results.csv'}.\")\n",
    "\n",
    "\n",
    "def get_path_w_extension(\n",
    "    path: str, extension: str, limit: int, ignore_files: list = []\n",
    "):\n",
    "    \"\"\"Finds files that match extensions and returns a list of those files that match up to thhe limit while ignoring files in the ignore_files list.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to search for files in.\n",
    "        extension (str): Type of extension to look for.\n",
    "        ignore_files (list, optional): Specify list of files that will be ignored. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        list: List of paths to files with extensions.\n",
    "    \"\"\"\n",
    "    logging.debug(f\"Path: {path}\")\n",
    "    logging.debug(f\"Extension: {extension}\")\n",
    "    if isinstance(path, str):\n",
    "        abs_path = os.path.abspath(path)\n",
    "    elif isinstance(path, Path):\n",
    "        abs_path = path.absolute()\n",
    "    else:\n",
    "        raise ValueError(f\"Error: Path {path} is not valid.\")\n",
    "\n",
    "    if not os.path.exists(abs_path):\n",
    "        raise ValueError(f\"Error: Path {abs_path} does not exist.\")\n",
    "\n",
    "    if os.path.isdir(abs_path):\n",
    "        pt_files = []\n",
    "        for root, dirs, files in os.walk(abs_path):\n",
    "            for file in files:\n",
    "                if (\n",
    "                    file.endswith(extension)\n",
    "                    and os.path.basename(file) not in ignore_files\n",
    "                ):\n",
    "                    pt_files.append(os.path.join(root, file))\n",
    "        if len(pt_files) <= limit and len(pt_files) > 0:\n",
    "            return pt_files\n",
    "        if len(pt_files) > limit:\n",
    "            raise ValueError(\n",
    "                f\"Error: Given limit: {limit} while number of files found with {extension} extension in directory {abs_path} is {len(pt_files)}\"\n",
    "            )\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Error: No {extension} files found in directory {abs_path}.\"\n",
    "            )\n",
    "    elif os.path.isfile(abs_path) and abs_path.endswith(extension):\n",
    "        return [abs_path]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Error: Path {abs_path} is not a valid directory or {extension} file.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def register_model(experiment_name: str, model_name: str, save_dir: Path):\n",
    "    \"\"\"Registers a model with mlflow\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of Mlfow experiment\n",
    "        model_name (str): Name that will be registered with Mlflow\n",
    "        save_dir (Path): Path object where the results of the Yolo model are saved. I.e 'runs' directory\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    logging.debug(f\"Save Directory: {save_dir}\")\n",
    "\n",
    "    '''model_path = get_path_w_extension(\n",
    "        path=save_dir, extension=\".pt\", limit=1, ignore_files=[\"last.pt\"]\n",
    "    )[0]'''\n",
    "    model_path = f\"{save_dir}/weights/best.pt\"\n",
    "    artifacts = {\"path\": model_path}\n",
    "\n",
    "    model = YoloWrapper()\n",
    "\n",
    "    exp_id = get_experiment_id(experiment_name)\n",
    "\n",
    "    #cloudpickle.register_pickle_by_value(model_wrapper)\n",
    "\n",
    "    with mlflow.start_run(experiment_id=exp_id) as run:\n",
    "        # Log some params\n",
    "        with open(save_dir / \"args.yaml\", \"r\") as param_file:\n",
    "            params = yaml.safe_load(param_file)\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        log_metrics(save_dir, True)\n",
    "        mlflow.log_artifact(f\"{save_dir}/weights/best.pt\")\n",
    "        pip_reqs = read_lines(\"requirements.txt\")\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=model,\n",
    "            pip_requirements=pip_reqs,\n",
    "            artifacts=artifacts,\n",
    "            registered_model_name=model_name,\n",
    "        )\n",
    "        run_id = run.info.run_uuid\n",
    "        experiment_id = run.info.experiment_id\n",
    "        mlflow.end_run()\n",
    "        logging.info(f\"artifact_uri = {mlflow.get_artifact_uri()}\")\n",
    "        logging.info(f\"runID: {run_id}\")\n",
    "        logging.info(f\"experiment_id: {experiment_id}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-05 11:43:12,931: INFO: common: yaml file: config\\config.yaml loaded sucessfully]\n",
      "[2024-08-05 11:43:12,931: INFO: common: yaml file: params.yaml loaded sucessfully]\n",
      "[2024-08-05 11:43:12,936: INFO: common: created directory at artifacts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/05 11:43:13 INFO mlflow.types.utils: Unsupported type hint: <class 'dict'>, skipping schema inference\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 157.72it/s]\n",
      "c:\\Users\\aravi\\anaconda3\\envs\\MLOPs\\lib\\site-packages\\_distutils_hack\\__init__.py:11: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aravi\\anaconda3\\envs\\MLOPs\\lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'yolov8ndet' already exists. Creating a new version of this model...\n",
      "2024/08/05 11:43:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: yolov8ndet, version 10\n",
      "Created version '10' of model 'yolov8ndet'.\n",
      "2024/08/05 11:43:14 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/05 11:43:14 INFO mlflow.tracking._tracking_service.client: 🏃 View run placid-mouse-301 at: http://127.0.0.1:5000/#/experiments/275689030631216605/runs/6c14248dc6b24815a9929b85262980e4.\n",
      "2024/08/05 11:43:14 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/275689030631216605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-05 11:43:14,521: INFO: 741786398: artifact_uri = mlflow-artifacts:/0/25b7111d874d4038a98a45bf411f2336/artifacts]\n",
      "[2024-08-05 11:43:14,521: INFO: 741786398: runID: 6c14248dc6b24815a9929b85262980e4]\n",
      "[2024-08-05 11:43:14,521: INFO: 741786398: experiment_id: 275689030631216605]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/05 11:43:14 INFO mlflow.tracking._tracking_service.client: 🏃 View run bouncy-bear-760 at: http://127.0.0.1:5000/#/experiments/0/runs/25b7111d874d4038a98a45bf411f2336.\n",
      "2024/08/05 11:43:14 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x = ConfigurationManager()\n",
    "    trainlog = x.get_train_log_config()\n",
    "    runs_f = get_highest_train_folder(\"runs/detect\")\n",
    "    dirr = Path(f\"runs/detect/{runs_f}\")\n",
    "\n",
    "\n",
    "    register_model(experiment_name=trainlog.experiment_name,model_name=trainlog.model_name,save_dir=dirr)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOPs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
